import numpy as np
import optuna
from optuna.pruners import ThresholdPruner
from tqdm import tqdm
import sys
import torch
from torch.nn import Module
from sklearn import metrics
import torch.optim as optim
from torch.optim.lr_scheduler import LambdaLR
from torch.utils.data.dataloader import DataLoader


from transformers import get_linear_schedule_with_warmup,AutoTokenizer, DataCollatorWithPadding

from datasets import DatasetDict, load_from_disk

from NewsArticleSorting.NASException import NASException
from NewsArticleSorting.NASLogger import logging

from NewsArticleSorting.NASEntity.NASArtifactEntity import DataPreprocessingArtifact, ModelTrainerArtifact
from NewsArticleSorting.NASEntity.NASConfigEntity import ModelTrainingConfig
from NewsArticleSorting.NASEntity.NASModels import NASModel


class NASModelTraining:
    """
    Class Name: NASModelTraining
    Description: Includes all the methods that are needed to create a NLP model to classify news articles.

    Written By: Jobin Mathew
    Interning at iNeuron Intelligence
    Version: 1.0
    """

    def __init__(self, 
                data_preprocessing_artifact:DataPreprocessingArtifact,
                model_training_config: ModelTrainingConfig) -> None:
        try:
            logging.info( f"{'*'*20}Model Training log started {'*'*20}")
            self.data_preprocessing_artifact = data_preprocessing_artifact
            self.model_training_config = model_training_config
            

        except Exception as e:
            raise NASException(e, sys) from e
    

    def tokenizer_function(self, example: DatasetDict)-> dict:
        """
        Method Name: tokenizer_function
        parameter: example - a DatasetDict record which will be passed during the map method of a DatasetDict object.

        Description: This method tokenizes the input text so that it produces input which can be passed to the model

        return: dict - a dictionary of the tokenized input text
        """
        # Tokenizing the text
        return self.tokenizer(example[self.model_training_config.input_feature],
                padding=self.model_training_config.padding_type,
                truncation=self.model_training_config.truncation,
                max_length=self.model_training_config.max_seq_length,
                )

    
    def loss_fn(self, outputs, targets)->torch.Tensor:
        """
        Method Name: loss_fn
        parameter: outputs - the outputs that are generated by the NLP model
                   targets - the expected or true outputs

        Description: This method returns the categorical cross entropy loss between generated output and True output

        return: torch.Tensor - the Categorical Cross-Entropy loss
        """
        # Implementing the Categorical CrossEntropy Loss
        softmax_output = torch.softmax(outputs, dim=1)
        log_softmaxed_output = torch.log(softmax_output)
        multiplied_result = torch.mul(log_softmaxed_output, targets)

        loss = torch.sum(multiplied_result)*(-1)

        return loss

    
    def train_fn(self, data_loader: DataLoader, model: Module, optimizer: optim, device: torch.device, scheduler: LambdaLR):
        """
        Method Name: train_fn
        parameter: data_loader - an object of pytorch'sDataLoader class which helps in loading data to the pytorch model
                   model - pytorch model which willperform classification
                   optimizer - optimizer object which ensures that during training the model improves in its task
                   device - where training is to be done(cpu/gpu)
                   scheduler - object which decides how learning rate is scheduled during training

        Description: This method trains the NLP model based on the data passed through dataloader
        """
        # Setting the model for training
        model.train()
        
        # Iterating through all the batches in the dataset
        for index, data in tqdm(enumerate(data_loader), total=len(data_loader)):

            # obtaining the inputs from the tokenized text and sending then to the device selected for training
            input_ids = data["input_ids"]
            attention_mask = data["attention_mask"]
            labels = data["labels"]

            input_ids = input_ids.to(device, dtype=torch.long)
            attention_mask = attention_mask.to(device, dtype=torch.long)
            labels = labels.to(device, dtype=torch.long)

            # Initiating the optimizer to zero for each iteration
            optimizer.zero_grad()
            # Obtaining the ouput from tihe model and calculating loss
            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            loss = self.loss_fn(outputs=outputs, targets=labels)
            
            # Backward Propogation
            loss.backward()
            # Optimizing and scheduling the learning rate.
            optimizer.step()
            scheduler.step()
        logging.info(f"the loss at the end of training epoch:{loss}")
    
    def eval_fn(self, data_loader, model, device):
        """
        Method Name: eval_fn
        parameter: data_loader - an object of pytorch's DataLoader class which helps in loading data to the pytorch model
                   model - pytorch model which will perform classification
                   device - where evaluation of model is to be done(cpu/gpu)


        Description: This method evaluates the NLP model based on the data passed through dataloader
        """
        # Setting the model for evaluation
        model.eval()

        fin_targets = []
        fin_outputs = []

        # Ensuring no gradients are calculated during evaluation.
        with torch.no_grad():
            # Iterating through all the batches in the dataset
            for index, data in tqdm(enumerate(data_loader), total=len(data_loader)):
                
                # obtaining the inputs from the tokenized text and sending then to the device selected for training
                input_ids = data["input_ids"]
                attention_mask = data["attention_mask"]
                labels = data["labels"]

                input_ids = input_ids.to(device, dtype=torch.long)
                attention_mask = attention_mask.to(device, dtype=torch.long)
                labels = labels.to(device, dtype=torch.long)

                # Obtaining the ouput from tihe model
                outputs = model(input_ids=input_ids, attention_mask=attention_mask)

                fin_targets.extend(labels.cpu().detach().numpy().tolist())
                fin_outputs.extend(torch.softmax(outputs, dim=1).cpu().detach().numpy().tolist())

        return fin_outputs, fin_targets 


    def load_from_disk(self)-> DatasetDict:
        """
        Method Name: load_from_disk

        Description: This method loads the DatasetDict object from  the disk

        returns: DatasetDict - the data to train, validate and test the model on.
        """
        try:
            # obtaining the data in the DatasetSict format from the disk
            nas_dataset = load_from_disk(self.data_preprocessing_artifact.train_dir_path)
            logging.info(f"The dataset is loaded from {self.data_preprocessing_artifact.train_dir_path}")
            return nas_dataset
        except Exception as e:
            raise NASException(e, sys) from e   


    def objective(self, trial: optuna.Trial)-> float:
        """
        Method Name: objective
        parameter: trial - the optuna's trial object which keeps track of relevant hyperparameter tuning results

        Description: This method is used by optuna the choose the model with the best metric chosen for evaluation. 

        returns: float - the accuracy generated for a particular optuna trial
        """
        # Setting the device based on whether GPU for DL training is available or not   
        device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")

        # Model name for model, tokenizer that will be selected for hyperparameter tuning training
        model_name = trial.suggest_categorical("model_name", self.model_training_config.models)

        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer)

        model = NASModel(model_name=model_name)
        model.to(device)

        # selecting the weight decay rate for model parameter
        param_optimizer = list(model.named_parameters()) 
        no_decay = ["bias", "LayerNorm.bias", "LayerNorm.weight"]

        optimizer_parameters = [
            {
                "params": [
                    p for n, p in param_optimizer if not any(nd in n for nd in no_decay)
                ],
                "weight_decay": 0.001,
            },
            {
                "params": [
                    p for n, p in param_optimizer if any(nd in n for nd in no_decay)
                ],
                "weight_decay": 0.0,
            },
        ]

        # Obtaining optimizer and learning rate value for hyperparameter tuning
        optimizer_name = trial.suggest_categorical("optimizer_name", self.model_training_config.optimizers)
        lr = trial.suggest_float("lr", float(self.model_training_config.learning_rate_start),
                                    float(self.model_training_config.learning_rate_end), log=True)

        optimizer = getattr(optim, optimizer_name)(optimizer_parameters, lr=lr)
        
        # Obtaining the DatasetDict that will be used for training
        nas_dataset = self.load_from_disk()

        # Performing tokenization
        nas_dataset = nas_dataset.map(self.tokenizer_function, batched=True, remove_columns=['Text'])

        # Using DataLoader to load the data to the model
        train_data_loader = DataLoader(
            nas_dataset['train'],
            batch_size = self.model_training_config.train_batch_size,
            collate_fn=data_collator
        )

        val_data_loader = DataLoader(
            nas_dataset['validation'],
            batch_size=self.model_training_config.train_batch_size,
            collate_fn=data_collator
        )

        # Setting up the scheduler for learning rate modification during training
        num_train_steps=int(len(train_data_loader)*self.model_training_config.hyperparameter_tuning_epochs)

        scheduler = get_linear_schedule_with_warmup(
                optimizer,
                num_warmup_steps=0,
                num_training_steps=num_train_steps
            )

        # Performing training and calculating accuracy forthe validation dataset
        for epoch in range(self.model_training_config.hyperparameter_tuning_epochs):
            self.train_fn(train_data_loader, model, optimizer, device, scheduler)
            outputs, targets = self.eval_fn(val_data_loader, model, device)
            outputs = np.array(outputs)>=np.max(outputs,axis=1).reshape(-1,1)
            accuracy = metrics.accuracy_score(targets, outputs)
        
            trial.report(accuracy, epoch)

            # this ensured that is some condition is not met during training then it will aboandon the hyerparameter loop
            if trial.should_prune():
                raise optuna.TrialPruned()

        return accuracy    

    def train_best_model(self, model_name: str, optimizer_name: str, lr: float)-> float:
        """
        Method Name: train_best_model
        parameter: model_name - The name of the model we will be used
                   optimizer_name - the optimizer name that we will be used
                   lr - the learning rate that will be used

        Description: This method the model with the best accuracy that was obtained during hyper-parameter tuning. 

        returns: float - the accuracy generated for the best model that was trained
        """
        try:
            # Setting the device based on whether GPU for DL training is available or not   
            device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
            
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

            # Model name for model, tokenizer that will be selected for training
            model = NASModel(model_name=model_name)
            model.to(device)

            # selecting the weight decay rate for model parameter
            param_optimizer = list(model.named_parameters()) 
            no_decay = ["bias", "LayerNorm.bias", "LayerNorm.weight"]

            optimizer_parameters = [
                {
                    "params": [
                        p for n, p in param_optimizer if not any(nd in n for nd in no_decay)
                    ],
                    "weight_decay": 0.001,
                },
                {
                    "params": [
                        p for n, p in param_optimizer if any(nd in n for nd in no_decay)
                    ],
                    "weight_decay": 0.0,
                },
            ]
            # Obtaining optimizer and learning rate value for hyperparameter tuning

            optimizer = getattr(optim, optimizer_name)(optimizer_parameters, lr=lr)
            
            # Obtaining the DatasetDict that will be used for training
            nas_dataset = self.load_from_disk()

            # Performing tokenization
            nas_dataset = nas_dataset.map(self.tokenizer_function, batched=True, remove_columns=['Text'])

            # Using DataLoader to load the data to the model
            train_data_loader = DataLoader(
                nas_dataset['train'],
                batch_size = self.model_training_config.train_batch_size,
                collate_fn=data_collator
            )

            val_data_loader = DataLoader(
                nas_dataset['validation'],
                batch_size=self.model_training_config.train_batch_size,
                collate_fn=data_collator
            )

            # Setting up the scheduler for learning rate modification during training
            num_train_steps=int(len(train_data_loader)*self.model_training_config.num_train_epochs)

            scheduler = get_linear_schedule_with_warmup(
                    optimizer,
                    num_warmup_steps=0,
                    num_training_steps=num_train_steps
                )

            best_accuracy = 0
            # Performing training and calculating accuracy forthe validation dataset
            for epoch in range(self.model_training_config.num_train_epochs):

                self.train_fn(train_data_loader, model, optimizer, device, scheduler)
                outputs, targets = self.eval_fn(val_data_loader, model, device)
                outputs = np.array(outputs)>=np.max(outputs,axis=1).reshape(-1,1)
                accuracy = metrics.accuracy_score(targets, outputs)
                
                print(f"Accuracy Score = {accuracy}")

                if accuracy > best_accuracy:
                    
                    if accuracy >= self.model_training_config.base_accuracy:
                        torch.save(model.state_dict(), self.model_training_config.trained_model_path)
                    best_accuracy = accuracy

            logging.info(f"Accuracy of the best model trained: {best_accuracy}")
            return best_accuracy

        except Exception as e:
            raise NASException(e,sys) from e

    def initiate_model_training(self)-> ModelTrainerArtifact:
        """
        Method Name: initiate_model_training

        Description: This method Combines all the relevant methods to achieve model training. 

        returns: ModelTrainerArtifact - contains all the relevant information for further model evaluation
        """
        try:

            # Creating an optuna study to perform model hyperparameter tuning
            study = optuna.create_study(direction="maximize", pruner=ThresholdPruner(lower=0.2))
            # Performing hyperparameter tuning
            study.optimize(lambda trial: self.objective(trial), n_trials=self.model_training_config.no_of_models_to_check)

            # Obtaining the parameters for best model that was trained
            best_parameters = study.best_trial.params
            logging.info(f"the best parameters which were tuned are: {best_parameters}")

            # Obtaining accuracy of best trained model
            accuracy = self.train_best_model(model_name=best_parameters['model_name'],
                                    optimizer_name=best_parameters['optimizer_name'],
                                    lr=best_parameters['lr'])

            # Checking to see if model accuracy is above accepted base accuracy 
            if accuracy >= self.model_training_config.base_accuracy:
                is_trained = True
                message = f" The relevant model is trained at {self.model_training_config.trained_model_path} with accuracy {accuracy}."
                
            else:
                is_trained = False
                message = f" The relevant model is not trained as accuracy was only {accuracy}."
            
            logging.info(message)

            model_training_artifact = ModelTrainerArtifact(
                is_trained=is_trained,
                message=message,
                trained_model_path=self.model_training_config.trained_model_path,
                model_name=best_parameters['model_name'],
                optimizer=best_parameters['optimizer_name'],
                lr=best_parameters['lr']
            )
            logging.info(f"Model Training Artifact: {model_training_artifact}")

            return model_training_artifact

        except Exception as e:
            raise NASException(e, sys) from e